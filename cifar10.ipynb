{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "4dc5288e-626c-4649-9eac-fe0ad0d57dd0"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wassim/anaconda/anaconda3/envs/deeplearning/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "5546f12a-176f-4435-b0ce-fb93952da7a2"
    }
   },
   "outputs": [],
   "source": [
    "CIFAR_DIR = 'cifar-10-batches-py/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "1ab6d2cb-a7d6-4c1d-8d09-c58d69d52d41"
    }
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
    "    return cifar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "e0ccf69f-5564-41f7-a836-cb9d541e1fc3"
    }
   },
   "outputs": [],
   "source": [
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "336e7acf-0601-441a-9a53-f920ea27bd45"
    }
   },
   "outputs": [],
   "source": [
    "all_data = [0,1,2,3,4,5,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "74d4f8c4-c110-4ab2-9a1d-d507f977b452"
    }
   },
   "outputs": [],
   "source": [
    "for i,direc in zip(all_data,dirs):\n",
    "    all_data[i] = unpickle(CIFAR_DIR+direc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "39afb696-0536-40a7-9231-c8bf3f332a07"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n"
     ]
    }
   ],
   "source": [
    "meta_data = all_data[0]\n",
    "print(meta_data[b'label_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "994c3c0e-5d84-40f7-a11d-b1b00267d7cd"
    }
   },
   "outputs": [],
   "source": [
    "all_training_data = np.concatenate([all_data[1][b\"data\"] ,all_data[2][b\"data\"] ,all_data[3][b\"data\"] ,all_data[4][b\"data\"] ,all_data[5][b\"data\"] ])\n",
    "all_training_labels = np.concatenate([all_data[1][b\"labels\"] ,all_data[2][b\"labels\"] ,all_data[3][b\"labels\"] ,all_data[4][b\"labels\"] ,all_data[5][b\"labels\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "90790020-b7ec-4d1a-bd43-2dba53424a04"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "50000\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(all_training_labels.shape)\n",
    "print(len(all_training_labels))\n",
    "print(all_training_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "e5ee7fbe-86c2-4818-953c-1041e05b6f44"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    z = np.zeros((len(x), 10))\n",
    "    z[list(np.indices((len(x),))) + [x]] = 1\n",
    "    return z\n",
    "\n",
    "all_training_labels = one_hot_encode(all_training_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "053a3b90-9481-4656-b6a5-f27e171bfd00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "50000\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(all_training_labels.shape)\n",
    "print(len(all_training_labels))\n",
    "print(all_training_labels.dtype)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "a1839266-8e54-42c6-ba27-b34fc7527191"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(all_training_data.shape)\n",
    "\n",
    "all_training_data = tf.reshape(all_training_data,shape=[50000, 3, 32, 32])\n",
    "\n",
    "print(all_training_data.shape)\n",
    "\n",
    "all_training_data = tf.transpose(all_training_data,perm=[0,2,3,1])\n",
    "\n",
    "all_training_data = tf.cast(all_training_data,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbpresent": {
     "id": "f55dad96-21b4-4b3c-8bfd-d775acc0ef2c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::=> training data shapes\n",
      "(50000, 32, 32, 3)\n",
      "DEBUG::=> training labels shapes\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbpresent": {
     "id": "8296726d-9bbe-4c76-8ebf-a6bc9d17f909"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-67558c58e9a0>:1: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n"
     ]
    }
   ],
   "source": [
    "data_dataset = tf.contrib.data.Dataset.from_tensor_slices(all_training_data)\n",
    "labels_dataset = tf.contrib.data.Dataset.from_tensor_slices(all_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbpresent": {
     "id": "a3640680-ef11-4f14-80c9-871400a7b51a"
    }
   },
   "outputs": [],
   "source": [
    "batched_data_dataset = data_dataset.batch(100)\n",
    "batched_labels_dataset = labels_dataset.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "db37faad-d7d6-47f0-a729-5f292aa08b5e"
    }
   },
   "outputs": [],
   "source": [
    "data_iterator = batched_data_dataset.make_initializable_iterator()\n",
    "label_iterator = batched_labels_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "48d33e45-cbac-4f0a-ac17-0f9847be2a60"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::=> data iterator output shapes\n",
      "(?, 32, 32, 3)\n",
      "DEBUG::=> label iterator output shapes\n",
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "print('DEBUG::=> data iterator output shapes')\n",
    "print(data_iterator.output_shapes)\n",
    "print('DEBUG::=> label iterator output shapes')\n",
    "print(label_iterator.output_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2688b7d9-ea2c-4937-b67d-875e17247f0b"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "f11e60cb-cbc5-4dea-becc-686db055fb88"
    }
   },
   "outputs": [],
   "source": [
    "#Hyper Parameter  \n",
    "learning_rate = 0.01\n",
    "num_epochs = 15\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "num_channels = 3 \n",
    "\n",
    "#Network Parameters\n",
    "num_input = 3072\n",
    "num_classes = 1000\n",
    "dropout_keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "050e726f-537b-4167-aa6f-0c1df4382adf"
    }
   },
   "outputs": [],
   "source": [
    "def create_placeholders(height,width,num_channels,num_classes):\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[None,32,32,3],name='X-data')\n",
    "    Y = tf.placeholder(dtype=tf.float32,shape=[None,10],name='Y-labels')\n",
    "    return X ,Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "d7fe627b-0466-4394-8a90-037af74fa37b"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters():\n",
    "\n",
    "    # Create them \n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=[3,3,3,32], stddev=0.05),name='W1')\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=[3,3,32,64], stddev=0.05),name='W2')\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=[3,3,64,32], stddev=0.05),name='W2')\n",
    "    W4 = tf.Variable(tf.truncated_normal(shape=[3,3,32,16], stddev=0.05),name='W2')\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\" :W3,\n",
    "                  \"W4\": W4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "2c610dae-9b31-4813-bb4d-7ad8747c6935"
    }
   },
   "outputs": [],
   "source": [
    "# Forward Pas\n",
    "\"\"\"\n",
    "    our forward pass through the network will consist of this \n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\"\"\"\n",
    "def forward_pass(X,weights):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    W2 = weights['W2']\n",
    "    W3 = weights['W3']\n",
    "    W4 = weights['W4']\n",
    "    \n",
    "\n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME',name = 'Z1')\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1,name = 'A1')\n",
    "    # MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1, 8, 8, 1], strides = [1, 8, 8, 1], padding='SAME',name = 'P1')\n",
    "    \n",
    "    \n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME',name = 'Z2')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2,name = 'A2')\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME',name = 'P2')\n",
    "    \n",
    "    \n",
    "  \n",
    "    Z3 = tf.nn.conv2d(P2, W3, strides=[1, 1, 1, 1], padding='SAME',name = 'Z3')\n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(Z3,name = 'A3')\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P3 = tf.nn.max_pool(A3, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME',name = 'P3')\n",
    "    \n",
    "    \n",
    "    Z4 = tf.nn.conv2d(P3, W4, strides=[1, 1, 1, 1], padding='SAME',name = 'Z4')\n",
    "    # RELU\n",
    "    A4 = tf.nn.relu(Z4,name = 'A4')\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P4 = tf.nn.max_pool(A4, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME',name = 'P4')\n",
    "    \n",
    "    \n",
    "    # FLATTEN\n",
    "    P = tf.contrib.layers.flatten(P4)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z6 = tf.contrib.layers.fully_connected(P, 1000, activation_fn=None)\n",
    "    Z7 = tf.contrib.layers.fully_connected(P, 512, activation_fn=None)\n",
    "    Z8 = tf.contrib.layers.fully_connected(P, 256, activation_fn=None)\n",
    "    Z9 = tf.contrib.layers.fully_connected(P, 128, activation_fn=None)\n",
    "    Z10 = tf.contrib.layers.fully_connected(P, 64, activation_fn=None)\n",
    "    Z11 = tf.contrib.layers.fully_connected(P, 32, activation_fn=None)    \n",
    "    predictions = tf.contrib.layers.fully_connected(Z11,10,activation_fn=None)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "c9a9fe7c-b7d0-48e3-a769-0542c57a01ef"
    }
   },
   "outputs": [],
   "source": [
    "# Now that we have Z3 and our true labels, let's compute the cost\n",
    "\n",
    "def compute_cost(labels,Z6):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=Z6))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "3c6b4c0d-bbae-46df-9e83-5d44acfee416"
    }
   },
   "outputs": [],
   "source": [
    "# Finally let's create our model\n",
    "losses = []\n",
    "def model( num_epochs , learning_rate):\n",
    "    #Create placeholder \n",
    "\n",
    "    X, Y = create_placeholders(img_height,img_width,num_channels,num_classes)\n",
    "\n",
    "    parameters = initialize_parameters()\n",
    "    predictions = forward_pass(X,parameters)\n",
    "    print(parameters['W1'])\n",
    "    summary_pred =  tf.summary.histogram(name = 'predictions',values=predictions)\n",
    "    cost = compute_cost(Z6=predictions,labels=Y)\n",
    "    summary_cost = tf.summary.histogram(name = 'cost',values=cost)    \n",
    "    \n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    init_data_iterator = data_iterator.initializer\n",
    "    init_label_iterator = label_iterator.initializer\n",
    "    train_writer = tf.summary.FileWriter('logs')\n",
    "    summary_merger = tf.summary.merge_all()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(all_training_labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess :\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        sess.run(parameters)\n",
    "\n",
    "        for epoch in range(1,num_epochs):\n",
    "\n",
    "            sess.run(init_data_iterator)\n",
    "\n",
    "            sess.run(init_label_iterator)\n",
    "\n",
    "            for i in range(1,499):\n",
    "\n",
    "                mini_batch_data =  sess.run(data_iterator.get_next())\n",
    "      \n",
    "                mini_batch_labels = sess.run(label_iterator.get_next())\n",
    "\n",
    "                _ , loss_value,summary1,summary2,acc  = sess.run([train,cost,summary_pred,summary_cost,accuracy], feed_dict= {\n",
    "                    X : mini_batch_data,\n",
    "                    Y : mini_batch_labels\n",
    "                })\n",
    "\n",
    "                train_writer.add_summary(summary1)\n",
    "                train_writer.add_summary(summary2)\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "                losses.append(loss_value)\n",
    "\n",
    "                if i % 100 == 0 :\n",
    "                    print('Epoch : {} minbatch {} loss_value {} accuract {}'.format(epoch,i , loss_value,acc))\n",
    "                \n",
    "            print('Epoch Done ')\n",
    "            \n",
    "        print('Training Done')\n",
    "        print('Now Evaluating')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        train_writer.add_graph(sess.graph)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "869f5c60-438d-41c9-9216-f7082eab3259"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W1:0' shape=(3, 3, 3, 32) dtype=float32_ref>\n",
      "WARNING:tensorflow:From <ipython-input-23-4e75f693a566>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model(learning_rate=0.01,num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f7e6710b-4f45-4573-bdb5-e1816fa80583"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[300:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fbae2ef3-23b6-40a2-a9b1-a2d34dfd5e37"
    }
   },
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a7183815-f03e-4482-9ac0-bee77ce1d86c"
    }
   },
   "outputs": [],
   "source": [
    "all_testing_data = all_data[6][b'data']\n",
    "all_testing_labels = all_data[6][b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEBUG::=> Testing data shapes')\n",
    "print('all_testing_data shape {}'.format(all_testing_data.shape))\n",
    "print('all_testing_labels shape {}'.format(len(all_testing_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_labels = one_hot_encode(all_testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_testing_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_testing_data.shape)\n",
    "\n",
    "all_testing_data = tf.reshape(all_testing_data,shape=[10000, 3, 32, 32])\n",
    "\n",
    "print(all_testing_data.shape)\n",
    "\n",
    "all_testing_data = tf.transpose(all_testing_data,perm=[0,2,3,1])\n",
    "\n",
    "all_testing_data = tf.cast(all_testing_data,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
